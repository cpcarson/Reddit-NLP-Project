{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier,\\\n",
    "                                RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "\n",
    "df = pd.read_csv('./data/df_export.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit     0\n",
       "body         72\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning variables\n",
    "\n",
    "X = df['body']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data for training and testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipeline = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression(solver = 'lbfgs'))\n",
    "])\n",
    "params = [{\n",
    "    'tf__max_features' : [1000, 1250],\n",
    "    'tf__stop_words'   : [None, 'english'],\n",
    "    'tf__ngram_range'  : [(1, 1), (1, 2)],\n",
    "    'logreg__C'         : [.5, 1, 2]\n",
    "}]\n",
    "gs_log = GridSearchCV(log_pipeline, param_grid = params, \n",
    "                  cv = 5, n_jobs = -1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   15.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Logistic Regression/Tfidf', 'train score': 0.7597721886165058, 'test score': 0.7273692030139022, 'best params': {'logreg__C': 1, 'tf__max_features': 1250, 'tf__ngram_range': (1, 1), 'tf__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "gs_log.fit(X_train, y_train)\n",
    "log_score = {'estimator' : 'Logistic Regression/Tfidf',\n",
    "            'train score' : gs_log.score(X_train, y_train),\n",
    "            'test score' : gs_log.score(X_test, y_test),\n",
    "            'best params'    : gs_log.best_params_\n",
    "}\n",
    "model_scores.append(log_score)\n",
    "print(log_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(solver = 'lbfgs'))\n",
    "])\n",
    "params = [{\n",
    "    'vec__max_features' : [1000, 1250],\n",
    "    'vec__stop_words'   : [None, 'english'],\n",
    "    'vec__ngram_range'  : [(1, 1), (1, 2)],\n",
    "    'logreg__C'         : [.5, 1, 2]\n",
    "}]\n",
    "gs_log = GridSearchCV(log_pipeline, param_grid = params, \n",
    "                  cv = 5, n_jobs = -1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   14.4s finished\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Logistic Regression', 'train score': 0.7582510877639818, 'test score': 0.7223814071951608, 'best params': {'logreg__C': 0.5, 'vec__max_features': 1250, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "gs_log.fit(X_train, y_train)\n",
    "log_score = {'estimator' : 'Logistic Regression',\n",
    "            'train score' : gs_log.score(X_train, y_train),\n",
    "            'test score' : gs_log.score(X_test, y_test),\n",
    "            'best params'    : gs_log.best_params_\n",
    "}\n",
    "model_scores.append(log_score)\n",
    "print(log_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the test scores for Logistic Regression with both CountVectorizer and TFIDFVectorizer are almost identical. I'm going to continue using CountVectorizer because I am more familiar with its parameters. Also of note, both models liked the highest max_features parameter, so I will increase that hyperparameter to see if it yields better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "params = [{\n",
    "    'vec__max_features' : [1250, 1500],\n",
    "    'vec__stop_words'   : [None, 'english'],\n",
    "    'vec__ngram_range'  : [(1, 1), (1, 2)],\n",
    "    'nb__alpha'         : [.5, 1, 2]\n",
    "}]\n",
    "gs_nb = GridSearchCV(nb_pipeline, param_grid = params, \n",
    "                  cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  86 out of 120 | elapsed:    9.0s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Multinomial Naive Bayes', 'train score': 0.7301991580883653, 'test score': 0.7212140507269447, 'params': {'nb__alpha': 0.5, 'vec__max_features': 1500, 'vec__ngram_range': (1, 1), 'vec__stop_words': 'english'}}\n"
     ]
    }
   ],
   "source": [
    "gs_nb.fit(X_train, y_train)\n",
    "nb_score = {'estimator' : 'Multinomial Naive Bayes',\n",
    "            'train score' : gs_nb.score(X_train, y_train),\n",
    "            'test score' : gs_nb.score(X_test, y_test),\n",
    "            'params'    : gs_nb.best_params_\n",
    "}\n",
    "model_scores.append(nb_score)\n",
    "print(nb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the model seemed to do best with the maximum hyperparameter for max_features. I am going to keep increasing it until the performance boost bottoms out. Also of note, all three models performed best with the base ngram range of (1,1). I will continue using that ngram range from here on out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "params = [{\n",
    "    'vec__max_features': [1500, 1750],\n",
    "    'vec__stop_words'  : [None],\n",
    "    'vec__ngram_range' : [(1, 1)],\n",
    "    'rf__n_estimators' : [100, 250, 500],\n",
    "    'rf__max_features': ['auto'],\n",
    "    'rf__max_depth': [None, 3, 5],\n",
    "    'rf__min_samples_split' : [2, 3, 5]\n",
    "}]\n",
    "gs_rf = GridSearchCV(rf_pipeline, param_grid = params, \n",
    "                  cv = 3, n_jobs = -1, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 294 out of 324 | elapsed:  4.5min remaining:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Random Forest', 'train score': 0.9762991262513707, 'test score': 0.8150270614453996, 'best params': {'rf__max_depth': None, 'rf__max_features': 'auto', 'rf__min_samples_split': 5, 'rf__n_estimators': 250, 'vec__max_features': 1750, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "gs_rf.fit(X_train, y_train)\n",
    "rf_score = {'estimator' : 'Random Forest',\n",
    "            'train score' : gs_rf.score(X_train, y_train),\n",
    "            'test score' : gs_rf.score(X_test, y_test),\n",
    "            'best params'    : gs_rf.best_params_\n",
    "}\n",
    "model_scores.append(rf_score)\n",
    "print(rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('et', ExtraTreesClassifier()),\n",
    "])\n",
    "\n",
    "params = [{\n",
    "    'vec__max_features': [1750, 2000],\n",
    "    'vec__stop_words'  : [None],\n",
    "    'vec__ngram_range' : [(1, 1)],\n",
    "    'et__n_estimators' : [100, 250, 500],\n",
    "    'et__max_features': ['auto'],\n",
    "    'et__max_depth': [None, 3, 5], \n",
    "    'et__min_samples_split' : [2, 3, 5]\n",
    "}]\n",
    "gs_et = GridSearchCV(et_pipeline, param_grid = params, \n",
    "                  cv = 3, n_jobs = -1, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=-1)]: Done 122 out of 162 | elapsed:  1.5min remaining:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Extra Trees', 'train score': 0.9808624288089427, 'test score': 0.8256393929746365, 'best params': {'et__max_depth': None, 'et__max_features': 'auto', 'et__min_samples_split': 2, 'et__n_estimators': 500, 'vec__max_features': 2000, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "gs_et.fit(X_train, y_train)\n",
    "et_score = {'estimator' : 'Extra Trees',\n",
    "            'train score' : gs_et.score(X_train, y_train),\n",
    "            'test score' : gs_et.score(X_test, y_test),\n",
    "            'best params' : gs_et.best_params_\n",
    "}\n",
    "model_scores.append(et_score)\n",
    "print(et_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('ss', StandardScaler(with_mean = False)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = [{\n",
    "    'vec__max_features': [1750, 2000],\n",
    "    'vec__stop_words'  : [None],\n",
    "    'vec__ngram_range' : [(1, 1)],\n",
    "    'knn__n_neighbors' : [3, 4, 5],\n",
    "    'knn__p'           : [1, 2]\n",
    "}]\n",
    "gs_knn = GridSearchCV(knn_pipeline, param_grid = params, \n",
    "                  cv = 3, n_jobs = -1, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  36 | elapsed:   32.3s remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  36 | elapsed:   39.5s remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  36 | elapsed:  2.8min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  36 | elapsed:  2.9min remaining:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  3.1min finished\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...ki',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'vec__max_features': [1750, 2000], 'vec__stop_words': [None], 'vec__ngram_range': [(1, 1)], 'knn__n_neighbors': [3, 4, 5], 'knn__p': [1, 2]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'K Nearest Neighbor', 'train score': 0.827903357034207, 'test score': 0.6747320386288868, 'best params': {'knn__n_neighbors': 3, 'knn__p': 2, 'vec__max_features': 1750, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "knn_score = {'estimator' : 'K Nearest Neighbor',\n",
    "            'train score' : gs_knn.score(X_train, y_train),\n",
    "            'test score' : gs_knn.score(X_test, y_test),\n",
    "            'best params' : gs_knn.best_params_\n",
    "}\n",
    "model_scores.append(knn_score)\n",
    "print(knn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('bc', BaggingClassifier()),\n",
    "])\n",
    "\n",
    "params = [{\n",
    "    'vec__max_features': [1750, 2000],\n",
    "    'vec__stop_words'  : [None],\n",
    "    'vec__ngram_range' : [(1, 1)],\n",
    "    'bc__n_estimators' : [10, 50, 100]\n",
    "}]\n",
    "gs_bc = GridSearchCV(bc_pipeline, param_grid = params, \n",
    "                  cv = 3, n_jobs = -1, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  18 | elapsed:   32.8s remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  18 | elapsed:  2.5min remaining:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  18 | elapsed:  2.6min remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  18 | elapsed:  5.0min remaining:   59.7s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...imators=10, n_jobs=None, oob_score=False, random_state=None,\n",
       "         verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'vec__max_features': [1750, 2000], 'vec__stop_words': [None], 'vec__ngram_range': [(1, 1)], 'bc__n_estimators': [10, 50, 100]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_bc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Bagging Classifier', 'train score': 0.9808624288089427, 'test score': 0.8070678127984718, 'best params': {'bc__n_estimators': 100, 'vec__max_features': 2000, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "bc_score = {'estimator' : 'Bagging Classifier',\n",
    "            'train score' : gs_bc.score(X_train, y_train),\n",
    "            'test score' : gs_bc.score(X_test, y_test),\n",
    "            'best params' : gs_bc.best_params_\n",
    "}\n",
    "model_scores.append(bc_score)\n",
    "print(bc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "params = [{\n",
    "    'vec__max_features': [1750, 2000],\n",
    "    'vec__stop_words'  : [None],\n",
    "    'vec__ngram_range' : [(1, 1)],\n",
    "    'gb__n_estimators' : [100, 500, 750],\n",
    "    'gb__min_samples_split' : [2, 3, 5]\n",
    "}]\n",
    "gs_gb = GridSearchCV(gb_pipeline, param_grid = params, \n",
    "                  cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  54 | elapsed:   11.6s remaining:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  54 | elapsed:   39.3s remaining:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   46.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'vec__max_features': [1750, 2000], 'vec__stop_words': [None], 'vec__ngram_range': [(1, 1)], 'gb__n_estimators': [100, 500, 750], 'gb__min_samples_split': [2, 3, 5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Gradient Booster', 'train score': 0.7886377303760302, 'test score': 0.748275496126499, 'best params': {'gb__min_samples_split': 5, 'gb__n_estimators': 750, 'vec__max_features': 2000, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "gb_score = {'estimator' : 'Gradient Booster',\n",
    "            'train score' : gs_gb.score(X_train, y_train),\n",
    "            'test score' : gs_gb.score(X_test, y_test),\n",
    "            'best params' : gs_gb.best_params_\n",
    "}\n",
    "model_scores.append(gb_score)\n",
    "print(gb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('ss', StandardScaler(with_mean = False)),\n",
    "    ('svc', SVC()),\n",
    "])\n",
    "\n",
    "params = [{\n",
    "    'vec__max_features': [2000],\n",
    "    'vec__stop_words'  : [None],\n",
    "    'vec__ngram_range' : [(1, 1)],\n",
    "    'svc__kernel'      : ['rbf', 'poly'],\n",
    "    'svc__degree'      : [2],\n",
    "    'svc__C'           : [.5, 1]\n",
    "}]\n",
    "gs_svc = GridSearchCV(svc_pipeline, param_grid = params, \n",
    "                  cv = 3, n_jobs = -1, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:  2.9min remaining:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:  3.3min remaining:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:  3.5min remaining:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  3.9min finished\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...f', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'vec__max_features': [2000], 'vec__stop_words': [None], 'vec__ngram_range': [(1, 1)], 'svc__kernel': ['rbf', 'poly'], 'svc__degree': [2], 'svc__C': [0.5, 1]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Support Vector Classifier', 'train score': 0.8452368318652942, 'test score': 0.7516714422158548, 'best params': {'svc__C': 1, 'svc__degree': 2, 'svc__kernel': 'rbf', 'vec__max_features': 2000, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "svc_score = {'estimator' : 'Support Vector Classifier',\n",
    "            'train score' : gs_svc.score(X_train, y_train),\n",
    "            'test score' : gs_svc.score(X_test, y_test),\n",
    "             'best params' : gs_svc.best_params_\n",
    "}\n",
    "model_scores.append(svc_score)\n",
    "print(svc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest, while overfit, performed the best on the test dataset. I will instantiate a random forest model with a more comprehensive gridsearch through its hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_large_pipeline = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "params = [{\n",
    "    'vec__max_features': [2250, 2500],\n",
    "    'vec__stop_words'  : [None],\n",
    "    'vec__ngram_range' : [(1, 1)],\n",
    "    'rf__n_estimators' : [250, 300],\n",
    "    'rf__max_features': ['auto', 'log2'],\n",
    "    'rf__max_depth': [None],\n",
    "    'rf__min_samples_split' : [4, 5, 6],\n",
    "    'rf__min_samples_leaf': [1, 2, 3]\n",
    "    \n",
    "}]\n",
    "gs_rf_large = GridSearchCV(rf_large_pipeline, param_grid = params, \n",
    "                  cv = 3, n_jobs = 4, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=4)]: Done 216 out of 216 | elapsed: 24.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)\\...\n",
       "             iid='deprecated', n_jobs=4,\n",
       "             param_grid=[{'rf__max_depth': [None],\n",
       "                          'rf__max_features': ['auto', 'log2'],\n",
       "                          'rf__min_samples_leaf': [1, 2, 3],\n",
       "                          'rf__min_samples_split': [4, 5, 6],\n",
       "                          'rf__n_estimators': [250, 300],\n",
       "                          'vec__max_features': [2250, 2500],\n",
       "                          'vec__ngram_range': [(1, 1)],\n",
       "                          'vec__stop_words': [None]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf_large.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': 'Large Random Forest', 'train score': 0.979624323463865, 'test score': 0.8313700520004245, 'best params': {'rf__max_depth': None, 'rf__max_features': 'log2', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 6, 'rf__n_estimators': 300, 'vec__max_features': 2500, 'vec__ngram_range': (1, 1), 'vec__stop_words': None}}\n"
     ]
    }
   ],
   "source": [
    "rf_large_score = {'estimator' : 'Large Random Forest',\n",
    "            'train score' : gs_rf_large.score(X_train, y_train),\n",
    "            'test score' : gs_rf_large.score(X_test, y_test),\n",
    "            'best params'    : gs_rf_large.best_params_\n",
    "}\n",
    "model_scores.append(rf_large_score)\n",
    "print(rf_large_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(model_scores)\n",
    "df_scores = df_scores.drop(columns = ['best params', 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = gs_rf_large.best_estimator_['rf'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words = gs_rf_large.best_estimator_.named_steps.vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(importances, important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('./results/scores_dataframe.csv')\n",
    "coefs.to_csv('./results/word_coefs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
