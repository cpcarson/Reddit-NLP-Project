{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to scrape Reddit posts, adapted from code by Tim Book, GA intructor\n",
    "\n",
    "url = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "def get_comments(subreddit, length):\n",
    "    df_list = []\n",
    "    current_time = 1587430567\n",
    "    for i in range(length):\n",
    "        res = requests.get(\n",
    "            url,\n",
    "            params={\n",
    "                \"subreddit\": subreddit,\n",
    "                \"size\": 1000,\n",
    "                \"lang\": True,\n",
    "                \"before\": current_time\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            data = res.json()['data']\n",
    "        except:\n",
    "            pass\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df[[\"created_utc\", \"body\", \"subreddit\", \"author\"]]\n",
    "        df_list.append(df)\n",
    "        current_time = df.created_utc.min()\n",
    "    return pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to convert the comments into one list of tokenized words\n",
    "\n",
    "def convert_to_tokens(series):\n",
    "    ls = series.tolist()\n",
    "    lower_ls = [i.lower() for i in ls]\n",
    "    lists_of_words = [RegexpTokenizer('[a-z]\\w+').tokenize(i) for i in lower_ls]\n",
    "    small_list_of_words = [i for i in lists_of_words if i not in stopwords.words('english')]\n",
    "    big_list = []\n",
    "    for chunk in small_list_of_words:\n",
    "        big_list += chunk\n",
    "    return big_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to convert the comments into tokenized and lemmatized words\n",
    "\n",
    "appo = {\n",
    "    're' : 'are', 's' : 'is', 't' : 'not',\n",
    "    'd' : 'would', 've': 'had', 'll': 'will'\n",
    "}\n",
    "\n",
    "def clean_to_lemmas(text):\n",
    "    low = text.lower()\n",
    "    clean = low.replace('\\n', '')\n",
    "    tokens = RegexpTokenizer('\\w+').tokenize(clean)\n",
    "    words = ' '.join(tokens)\n",
    "    word = words.split(' ')\n",
    "    ls = [appo[word] if word in appo else word for word in word]\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lems = [lemmer.lemmatize(word) for word in ls]\n",
    "    clean_words = ' '.join(lems)\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining and Variable Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scraping 20,000 comments from the \"conservatives\" subreddit \n",
    "\n",
    "df_con = get_comments('conservatives', 20)\n",
    "\n",
    "# Scraping 20,000 comments from the \"socialism\" subreddit\n",
    "\n",
    "df_lib = get_comments('socialism', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the dataframes of scraped comments\n",
    "\n",
    "df_con.to_csv('./data/df_con.csv', index = False)\n",
    "df_lib.to_csv('./data/df_lib.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the individual subreddit dataframes\n",
    "\n",
    "df = pd.concat([df_lib, df_con], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc    0\n",
       "body           0\n",
       "subreddit      0\n",
       "author         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assessing null values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the Y variable to binary values\n",
    "\n",
    "df['subreddit'] = df['subreddit'].map({'conservatives' : 0,\n",
    "                                                'socialism' : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping comments that were removed or deleted\n",
    "\n",
    "df = df[df['author'] !='[deleted]']\n",
    "df = df[df['body']!='[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the combined dataframe\n",
    "\n",
    "df.to_csv('./data/data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the variables\n",
    "\n",
    "X = df['body']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing the comments by mapping the conversion function to the X variable\n",
    "\n",
    "X = X.map(clean_to_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teen vogue is actually laying it out man the kid have said this shit is broken and not working and instead of taking the just get a better job line teen vogue is like you are right here is specifically why you can get into the capitalism sell the rope to hang itself line if you want but we should be treating teen vogue a an ally'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the output of the cleaning and lemmatizing function\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for export with the lemmatized comments\n",
    "\n",
    "df_amazon_export = pd.concat([y, X], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the lemmatized data for modeling using AWS\n",
    "\n",
    "df_amazon_export.to_csv('./df_export.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
